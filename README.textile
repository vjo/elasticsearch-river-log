h1. Log River for Elasticsearch

Log River helps you to parse and index your log files in "Elasticsearch":http://www.elasticsearch.org/

*WARNING* : If you use this river in a multinode mode on differents servers, you need to ensure that the river can access files on the same mounting point. If not, when a node stop, the other node will _think_ that your local dir is empty and will *erase* all your docs.

h2. Versions

|_. Log River Plugin |_. ElasticSearch|_. Attachment Plugin|
|  master (0.0.1)   |    0.19.4      |       1.4.0        |

h2. Getting Started

h3. Installation

Just type :

<pre>
$ bin/plugin -install vjo/logriver/0.0.1-SNAPSHOT
</pre>

This will do the job...

<pre>
-> Installing vjo/logriver/0.0.1...
Trying https://github.com/downloads/vjo/logriver/logriver-0.0.1-SNAPSHOT.zip...
Downloading ...DONE
Installed logriver
</pre>

h3. Creating a Log River

We first create the river with the following properties :

* Log root path : <code>/tmp/log</code>
* Scan path every 5 minutes: 5 * 60 * 1000 = 300000 ms
* Get only files like <code>*.log</code> and <code>*.txt</code>
* Only index files older than 2 minutes: 2 * 60 * 1000 = 120000 ms 

<pre>
$ curl -XPUT 'localhost:9200/_river/my_log_test/_meta' -d '{
  "type": "log",
  "log": {
        "name": "My Log River",
        "url": "/var/stock/smtp",
        "update_rate": 30000,
        "includes": [ ".log", ".txt" ],
        "format" : "syslog",
        "operation": { "action" : "move",
                       "option" : "/tmp/"
                 },
        "last_modified" : 120000
  }
}'
</pre>

h3. Adding another FS river

We add another river with the following properties :

* FS URL : <code>/tmp2</code>
* Update Rate : every hour (60 * 60 * 1000 = 3600000 ms)
* Get only docs like <code>*.doc</code>, <code>*.xls</code> and <code>*.pdf</code>

By the way, we define to index in the same index/type as the previous one:

* index: <code>docs</code>
* type: <code>doc</code>

<pre>
$ curl -XPUT 'localhost:9200/_river/mynewriver/_meta' -d '{
  "type": "fs",
  "fs": {
	"name": "My tmp2 dir",
	"url": "/tmp2",
	"update_rate": 3600000,
	"includes": [ "*.doc" , "*.xls", "*.pdf" ]
  },
  "index": {
  	"index": "mydocs",
  	"type": "doc",
  	bulk_size: 50
  }
}'
</pre>

h3. Searching for docs

This is a common use case in elasticsearch, we want to search for something ;-)

<pre>
$ curl -XGET http://localhost:9200/docs/doc/_search -d '{
  "query" : {
    "text" : {
        "_all" : "I am searching for something !"
    }
  }
}'
</pre>


h2. Advanced

h3. Autogenerated mapping

When the FSRiver detect a new type, it creates automatically a mapping for this type.

<pre>
{
  "doc" : {
    "properties" : {
      "file" : {
        "type" : "attachment",
        "path" : "full",
        "fields" : {
          "file" : {
            "type" : "string",
            "store" : "yes",
            "term_vector" : "with_positions_offsets"
          },
          "author" : {
            "type" : "string"
          },
          "title" : {
            "type" : "string",
            "store" : "yes"
          },
          "name" : {
            "type" : "string"
          },
          "date" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "keywords" : {
            "type" : "string"
          },
          "content_type" : {
            "type" : "string"
          }
        }
      },
      "name" : {
        "type" : "string",
        "analyzer" : "keyword"
      },
      "pathEncoded" : {
        "type" : "string",
        "analyzer" : "keyword"
      },
      "postDate" : {
        "type" : "date",
        "format" : "dateOptionalTime"
      },
      "rootpath" : {
        "type" : "string",
        "analyzer" : "keyword"
      },
      "virtualpath" : {
        "type" : "string",
        "analyzer" : "keyword"
      }
    }
  }
}
</pre>

h3. Creating your own mapping (analyzers)

If you want to define your own mapping to set analyzers for example, you can push the mapping before starting the FS River.

<pre>
{
  "doc" : {
    "properties" : {
      "file" : {
        "type" : "attachment",
        "path" : "full",
        "fields" : {
          "file" : {
            "type" : "string",
            "store" : "yes",
            "term_vector" : "with_positions_offsets",
            "analyzer" : "french"
          },
          "author" : {
            "type" : "string"
          },
          "title" : {
            "type" : "string",
            "store" : "yes"
          },
          "name" : {
            "type" : "string"
          },
          "date" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "keywords" : {
            "type" : "string"
          },
          "content_type" : {
            "type" : "string"
          }
        }
      },
      "name" : {
        "type" : "string",
        "analyzer" : "keyword"
      },
      "pathEncoded" : {
        "type" : "string",
        "analyzer" : "keyword"
      },
      "postDate" : {
        "type" : "date",
        "format" : "dateOptionalTime"
      },
      "rootpath" : {
        "type" : "string",
        "analyzer" : "keyword"
      },
      "virtualpath" : {
        "type" : "string",
        "analyzer" : "keyword"
      }
    }
  }
}
</pre>

To send mapping to Elasticsearch, refer to the "Put Mapping API":http://www.elasticsearch.org/guide/reference/api/admin-indices-put-mapping.html

h3. Meta fields

FS River creates some meta fields :

|_.    Field      |_.       Description                                               |_.      Example                 |
| name            | Original file name                                                | mydocument.pdf                 |
| pathEncoded     | BASE64 encoded file path (for internal use)                       |112aed83738239dbfe4485f024cd4ce1|
| postDate        | Indexing date                                                     |1312893360000                   |
| rootpath        | BASE64 encoded root path (for internal use)                       |112aed83738239dbfe4485f024cd4ce1|
| virtualpath     | Relative path                                                     | mydir/otherdir                 |


h3. Advanced search

You can use meta fields to perform search on.

<pre>
$ curl -XGET http://localhost:9200/docs/doc/_search -d '{
  "query" : {
    "term" : {
        "name" : "mydocument.pdf"
    }
  }
}'
</pre>

h2. Behind the scene

h3. How it works ?

TO BE COMPLETED


